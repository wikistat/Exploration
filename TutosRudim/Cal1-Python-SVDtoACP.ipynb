{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"float:right; max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 200px; display: inline\" alt=\"Python\"/></a> [pour Statistique et Science des Données](https://github.com/wikistat/Intro-Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Anayse en Composantes Principales avec](http://wikistat.fr/pdf/st-m-explo-acp.pdf) <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 150px; display: inline\" alt=\"Python\"/></a> & <a href=\"http://scikit-learn.org/stable/#\"><img src=\"http://scikit-learn.org/stable/_static/scikit-learn-logo-small.png\" style=\"max-width: 180px; display: inline\" alt=\"Scikit-Learn\"/></a>\n",
    "**Résumé**: Ce calepin introduit l'utilisation de la librairie `scikit-learn` pour l'exploration statistique. Ceci est illustré par des exemples de mise en oeuvre de l'([ACP](http://wikistat.fr/pdf/st-m-explo-acp.pdf) sur des données \"jouet\" puis sur des images élémentaires de caractères et enfin sur des données économiques sous une la forme particulière d'un cube ou tableauà trois indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "### 1.1 `Scikit-learn` *vs.*  R\n",
    "L'objectif de ce tutoriel est d'introduire l'utilisation de la librairie `scikit-learn` de Python pour l'exploration de données multidimensionnelles. Seule l'utilisation directe des fonctions d'exploration est abordée. Se pose rapidement une question: quand utiliser `scikit-learn` de Python plutôt que R plus complet et plus simple d'emploi?\n",
    "\n",
    "Le choix repose sur les points suivants:\n",
    "- **Attention** cette librairie manipule des objets de classe `array` de `numpy` *chargés en mémoire* et donc de taille limitée par la RAM de l'ordinateur; de façon analogue R charge en RAM des objets de type `data.frame`.\n",
    "- **Attention** toujours, `scikit-learn` (0.18) ne reconnaît pas (ou pas encore ?) la classe `DataFrame` de `pandas`; `scikit-learn` utilise la classe `array` de `numpy`. C'est un problème pour la gestion de variables qualitatives complexes. Une variable binaire est simplement remplacée par un codage $(0,1)$ mais, en présence de plusieurs modalités, traiter celles-ci comme des entiers n'a pas de sens statistique et remplacer une variable qualitative par l'ensemble des indicatrices (*dummy variables* $(0,1)$) de ses modalités  complique l'interprétation statistique. \n",
    "- Les implémentations en Python de certains algorithmes dans `scikit-learn` sont aussi efficaces (*e.g.* $k$-means), voire beaucoup plus efficaces pour des données volumineuses car utilisent implicitement les capacités de parallélisation.\n",
    "- R offre beaucoup plus de possibilités pour une exploration, des recherches et comparaisons, des interprétations mais les capacités de parallélisation de Python sont nettement plus performantes. Plus précisément, l'introduction de nouvelles librairies n'est pas ou peu contraintes dans R, alors que celle de nouvelles méthodes dans `scikit-learn` se fait sous contrôle d'un groupe qui en contrôle la pertinence et l'efficacité. \n",
    "\n",
    "En conséquences:\n",
    "- Préférer R et ses libraires si la **présentation graphique** des résultats et leur **interprétation** est prioritaire.\n",
    "- Pour l'emploi de méthodes (analyse factorielle discriminante, canonique, positionnement multidimensionnel...) pas codées en Python.\n",
    "- Préférer Python et `scikit-learn` pour mettre au point une chaîne de traitements (*pipe line*) opérationnelle de l'extraction à une analyse privilégiant la prévision brute à l'interprétation et pour des données quantitatives ou rendues quantitatives (\"vectorisation\" de corpus de textes).\n",
    "\n",
    "### 1.2 Fonctions de `scikit-learn`\n",
    "La communauté qui développe cette librairie est très active, elle évolue rapidement.  Ne pas hésiter à consulter la [documentation](http://scikit-learn.org/stable/user_guide.html) pour des compléments. Voici une sélection de ses principales fonctionnalités.\n",
    "- Transformations (standardisation, discrétisation binaire, regroupement de modalités, imputations rudimentaires de données manquantes) , \"vectorisation\" de corpus de textes (encodage, catalogue, Tf-idf), images.\n",
    "- Exploration: ACP, classification non supervisée (mélanges gaussiens, propagation d'affinité, ascendante hiérarchique, SOM,...). Une fonction est aojutée pour l'Analyse des Correspondances.\n",
    "- Modélisation et apprentissage, voir le dépôt correspondant.\n",
    "\n",
    "\n",
    "### 1.3 ACP avec `scikit-learn`\n",
    "L'objectif est d'illustrer la mise en oeuvre de l'[analyse en composantes principales](http://wikistat.fr/pdf/st-m-explo-acp.pdf). Consulter la [documentation](http://scikit-learn.org/stable/user_guide.html) et ses nombreux [exemples](http://scikit-learn.org/stable/auto_examples/index.html) pour plus de détails sur l'utilisation de `scikit-learn`. \n",
    "\n",
    "La librairie `scikit-learn` a principalement été conçu en vue de l'analyse de signaux. Aussi, de nombreuses options de l'ACP ne sont pas disponibles, notamment les graphiques usuels (biplot, cercle des corrélations...). En revanche des résultats sont liés à la version probabiliste de l'ACP sous hypothèse d'une distribution gaussienne multidimensionnelle des données. \n",
    "\n",
    "**Attention**, l'ACP est évidemment centrée mais par réduite. L'option n'est pas prévue et les variables doivent être réduites (fonction `sklearn.preprocessing.scale`) avant si c'est nécessaire. L'attribut `transform` désigne les composantes principales, sous-entendu: transformation par réduction de la dimension; `n_components` fixe le nombre de composantes retenues, par défaut toutes; l'attribut `components_` contient les `n_components` vecteurs propres mais non normalisés, c'est-à-dire de norme carrée la valeur propre associée, et donc à utiliser pour représenter les variables.\n",
    "\n",
    "D'autres versions d'analyse en composantes principales sont proposées dans `Scikit-learn`: *kernel PCA, sparse PCA, ICA*...\n",
    "\n",
    "\n",
    "Plusieurs jeux de données élémentaires sont utilisés donyt celui \"jouet\" déjà vu en R afin de bien comprendre les sorties proposées par la fonction disponible.  L'autre ensemble de données est un problème classique et simplifié de [reconnaissance de caractères](http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits) qui est inclus dans la librairie `scikit-learn`.\n",
    "\n",
    "## 2.  ACP de données \"jouet\"\n",
    "Les données sont celles de l'exemple [introduction à l'ACP](http://wikistat.fr/pdf/st-l-des-multi): les notes en maths, français, physique et anglais de 9 lycéens virtuels. L'objectif est de contrôler les résultats en les comparant avec ceux obtenus avec R.\n",
    "\n",
    "C'est une façon générique de procéder à l'approche d'un nouveau logiciel ou de fonctionnalités inconnues: traiter des donées triviales dont les résultats de l'analyse sont parfaitement maîtrisés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construire la matrice de notes\n",
    "import pandas as pd\n",
    "note=[[6,6,5,5.5],[8,8,8,8],[6,7,11,9.5],[14.5,14.5,15.5,15],\n",
    "   [14,14,12,12.5],[11,10,5.5,7],[5.5,7,14,11.5],[13,12.5,8.5,9.5],\n",
    "   [9,9.5,12.5,12]]\n",
    "dat=pd.DataFrame(note,index=[\"jean\",\"alai\",\"anni\",\"moni\",\"didi\",\"andr\",\"pier\",\"brig\",\"evel\"],\n",
    "   columns=[\"Math\",\"Phys\",\"Fran\",\"Angl\"])\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des fonctions\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Valeurs propres et valeurs singulières de l'ACP non réduite\n",
    "**Attention** Les valeurs singulières sont celles de la décomposition de la matrice centrée par rapport aux  métriques usuelles: $(\\bar{X}, I_p, I_n)$ alors que le diviseur de la variance est celui d'une estimation sans biais: $(n-1)$.\n",
    "\n",
    "Contrairement à beaucoup de logiciels, l'ACP de `scikit-learn` n'est pas *réduite*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(dat).explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.singular_values_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les valeurs singulières associées à l'ACP sont celles de $(\\bar{X}, I_p, \\frac{1}{n-1}I_n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.singular_values_/np.sqrt(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour retrouver les valeurs propres de l'ACP à partir des valeurs singulières de la matrice centrée:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pca.singular_values_/np.sqrt(8))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Vecteurs propres de l'ACP non réduite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Composantes principales de l'ACP non réduite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.transform(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Comparer avec les résultats obtenus en R.\n",
    "\n",
    "Tous les autres résultats (contributions, cossinus carrés, corrélations variables facteurs...) et surtout les graphes (éboulis, plans factoriels...) sont à construire car aucune fonction n'est disponible comme dans `FactoMineR`. C'est partièlement fait dans le jeu de données suivant et complété (*biplot*) dans les calepins plus completes des cas d'usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Les données \"Caractères\"\n",
    "Il s'agit d'explorer les données issues de la pixellisation de tracés de caractères dont les procédés d'obtention et prétraitement sont décrits sur le [site de l'UCI](http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits) (Lichman, 2013). Les chiffres ont été saisies sur des tablettes à l'intérieur de cadres de résolution $500\\times 500$. Des procédures de normalisation,  ré-échantillonnage spatial puis de lissage ont été appliquées. Chaque caractère apparaît finalement discrétisé sous la forme d'une matrice $8\\times 8$ de pixels à 16 niveaux de gris et identifié par un label. Les données sont archivées sous la forme d'une matrice ou tableau à trois indices. Elles sont également archivées après vectorisation des images sous la forme d'une matrice à $p=64$ colonnes.\n",
    "\n",
    "L'étude du même type de données, mais nettement plus complexes (MNIST): 60 000 caractères représentés par des images de 784 pixels (26 $\\times$ 26) fait l'objet d'un autre calepin.\n",
    "\n",
    "## 3.1 Prise en main des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importations \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "# les données présentes dnas la librairie\n",
    "digits = datasets.load_digits()\n",
    "# Contenu et mode d'obtention\n",
    "print(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sous forme d'un cube d'images 1797 x 8x8\n",
    "print(digits.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sous forme d'une matrice 1797 x 64\n",
    "print(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label réel de chaque caractère\n",
    "print(digits.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un aperçu des empilements des images à décrire puis ensuite en principe à discriminer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_and_labels = list(zip(digits.images, \n",
    "   digits.target))\n",
    "for index, (image, label) in  enumerate(images_and_labels[:8]):\n",
    "     plt.subplot(2, 4, index + 1)\n",
    "     plt.axis('off')\n",
    "     plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "     plt.title('Chiffres: %i' % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Analyse en composantes principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "X=digits.data\n",
    "y=digits.target\n",
    "target_name=[0,1,2,3,4,5,6,7,8,9]\n",
    "# définition de la commande\n",
    "pca = PCA()\n",
    "# Estimation, calcul des composantes principales\n",
    "C = pca.fit(X).transform(X)\n",
    "# Décroissance de la variance expliquée\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagramme boîte des premières composantes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(C[:,0:20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Quelle dimension retenir en principe?\n",
    "\n",
    "Représentation des caractères dans le premier plan principal. \n",
    "\n",
    "La représentation des variables (pixels) et le *biplot* n'ont pas grand intérêt pour ces données. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(C[:,0], C[:,1], c=y, label=target_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le même graphique avec une légende mais moins de couleurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention aux indentations\n",
    "plt.figure()\n",
    "for c, i, target_name in zip(\"rgbcmykrgb\",[0,1,2,3,4,5,6,7,8,9], target_name):\n",
    "       plt.scatter(C[y == i,0], C[y == i,1], c=c, label=target_name)\n",
    "plt.legend()\n",
    "plt.title(\"ACP Digits\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphique en trois dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "ax = Axes3D(fig, elev=-150, azim=110)\n",
    "ax.scatter(C[:, 0], C[:, 1], C[:, 2], c=y, cmap=plt.cm.Paired)\n",
    "ax.set_title(\"ACP: trois premieres composantes\")\n",
    "ax.set_xlabel(\"Comp1\")\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.set_ylabel(\"Comp2\")\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.set_zlabel(\"Comp3\")\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Données \"cubiques\" de l'OCDE\n",
    "###  4.1 Introduction\n",
    "#### Objectif\n",
    "L'objectif de cette section  est l'exploration de données socio-économiques plus complexes. La principale spécificité de ces données est de se présenter  sous la forme d'un cube de données ou tableau à trois entrées: le numéro de ligne, le numéro de variable et l'année d'observation de cette variable. Après une description classique, la mise en oeuvre de l'analyse en composantes principales avec python nécessite un effort particulier afin de produire les graphes adaptés à la structure particulière des données. \n",
    "\n",
    "#### Les données\n",
    "Les données sont issues de l'Observatoire de l'OCDE.  Pour chaque pays membre et pour chacune des années  1975, 1977, 1979, 1981, on connaît les valeurs prises par les  variables suivantes qui sont toutes des \\emph{taux}~:\n",
    "- Taux brut de natalité, \n",
    "- Taux de chômage, \n",
    "- Pourcentage d'actifs dans le secteur primaire, \n",
    "- Pourcentage d'actifs dans le secteur secondaire, \n",
    "- produit intérieur brut (par habitant), \n",
    "- Formation brute de capital fixe (par habitant), \n",
    "- Hausse des prix, \n",
    "- Recettes courantes  (par habitant), \n",
    "- Mortalité infantile, \n",
    "- Consommation de protéines animales  (par habitant), \n",
    "- Consommation d'énergie  (par habitant).\n",
    "\n",
    "Elles sont disponibles dans le fichier: `ocdeR.dat`.\n",
    "\n",
    "Les mêmes variables sont donc observées, sur les mêmes pays ou individus à quatre dates différentes. Plusieurs stratégies d'analyse sont possibles (tableau moyen, tableaux concaténés, meilleur compromis ou double ACP). La plus adaptée pour ces données est de considérer les observations des variables pour chacun des individus:  pays $\\times$ années. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaiton des principals librairies et \n",
    "# Affichage des graphiques dans le notebook\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 2 Lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocde=pd.read_table(\"Data/ocdeR.dat\",sep='\\s+',index_col=0)\n",
    "ocde.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3  Statistiques élémentaires\n",
    "Consulter rapidement ces résultats; Que dire à propos de la symétrie des distributions, de leur normalité, des valeurs atypiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocde.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocde[\"CNRJ\"].hist(bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(ocde, alpha=0.2, figsize=(15, 15), diagonal='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Quel est le graphique ci-dessous? Que représentent les blocs dagonaux? Que dire des structures de corrélation? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 [Analyse en composantes principales](http://wikistat.fr/pdf/st-m-explo-acp.pdf)\n",
    "Chaque pays étant observé 4 fois, la principale difficulté technique est de faire apparaître cette structure chronologique dans les graphique afin d'illustrer la dynamique économique de la période considérée.\n",
    "\n",
    "**Q** Justifier la nécessité de réduire.\n",
    "\n",
    "**Q** Pourqoi toutes les variables sont des taux?\n",
    "\n",
    "#### Choix de dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "# réduction\n",
    "ocdeS=scale(ocde)\n",
    "pca = PCA()\n",
    "cpOcde = pca.fit_transform(ocdeS)\n",
    "# Eboulis\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(cpOcde)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Quel est le graphe ci-dessus. Que dire de la première composante? Quelle dimension choisir?\n",
    "\n",
    "#### Représentation des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord1=pca.components_[0]*np.sqrt(pca.explained_variance_[0])\n",
    "coord2=pca.components_[1]*np.sqrt(pca.explained_variance_[1])\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "for i, j, nom in zip(coord1,coord2, ocde.columns):\n",
    "    plt.text(i, j, nom)\n",
    "    plt.arrow(0,0,i,j,color='black')\n",
    "plt.axis((-1.2,1.2,-1.2,1.2))\n",
    "# cercle\n",
    "c=plt.Circle((0,0), radius=1, color='gray', fill=False)\n",
    "ax.add_patch(c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Interpréter chacun des deux premiers axes.\n",
    "\n",
    "**Exo** représenter le plan (2,3) et interpréter le 3ème axe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Représentation basique des individus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "for i, j, nom in zip(cpOcde[:,0], cpOcde[:,1], ocde.index):\n",
    "#    color = int(i/4)\n",
    "    plt.text(i, j, nom ,color=\"blue\")\n",
    "plt.axis((-5,7,-4,4))  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Représentation adaptée à ces données\n",
    "La structure particulière des données nécessite un graphique adapté. Ceci est en fait le **principal objectif** d'une *bonne exploration des données*: trouver la **représentation graphique** qui permet d'en comprendre toute la structure en une seule vue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patheffects as PathEffects\n",
    "\n",
    "comp_0 = 0\n",
    "comp_1 = 1\n",
    "\n",
    "cmap = plt.get_cmap(\"tab20\")\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "for i,k in enumerate(np.arange(0,cpOcde.shape[0],4)):\n",
    "    country =ocde.index[k]\n",
    "    xs = cpOcde[k:k+4,comp_0]\n",
    "    ys = cpOcde[k:k+4, comp_1]\n",
    "    ax.plot(xs,ys, color=cmap(i), marker=\".\", markersize=15)\n",
    "    txt = ax.text(xs[-4], ys[-4], country, horizontalalignment=\"left\", verticalalignment=\"top\",\n",
    "            color=cmap(i), fontweight=\"bold\", fontsize=15)\n",
    "    # Add black line around text\n",
    "    #txt.set_path_effects([PathEffects.withStroke(linewidth=1, foreground='black')])\n",
    "    ax.set_xlabel(\"PC%d\" %comp_0, fontsize=20)\n",
    "    ax.set_ylabel(\"PC%d\" %comp_1, fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Analyser les évolutions des économies des différents pays. Les remplacer dans la période considérée. "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "10",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
